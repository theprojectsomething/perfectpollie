# -*- coding: utf-8 -*-
"""GFPGAN_inference_PaperModel.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NE2wOUk-mw03CF71jVxbL78fN3cCGdeD

# GFPGAN Inference Demo (Paper Model)


[![arXiv](https://img.shields.io/badge/arXiv-Paper-<COLOR>.svg)](https://arxiv.org/abs/2101.04061)
[![GitHub Stars](https://img.shields.io/github/stars/TencentARC/GFPGAN?style=social)](https://github.com/TencentARC/GFPGAN)
[![download](https://img.shields.io/github/downloads/TencentARC/GFPGAN/total.svg)](https://github.com/TencentARC/GFPGAN/releases)

## GFPGAN - Towards Real-World Blind Face Restoration with Generative Facial Prior

GFPGAN is a blind face restoration algorithm towards real-world face images. <br>
It leverages the generative face prior in a pre-trained GAN (*e.g.*, StyleGAN2) to restore realistic faces while precerving fidelity. <br>

You may want to try an **imporoved** model without colorization. (It also does not require CUDA extensions). Please go to this [Colab Demo](https://colab.research.google.com/drive/1sVsoBd9AjckIXThgtZhGrHRfFI6UUYOo) for GFPGAN <a href="https://colab.research.google.com/drive/1sVsoBd9AjckIXThgtZhGrHRfFI6UUYOo"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="google colab logo"></a>.

**Limitations**: GFPGAN could not handle all the low-quality faces in the real world. Therefore, it may fail on your own cases.

###Enjoy! :-)

<img src="https://xinntao.github.io/projects/GFPGAN_src/gfpgan_teaser.jpg" width="800">

# Preparations
Before start, make sure that you choose
* Runtime Type = Python 3
* Hardware Accelerator = GPU

in the **Runtime** menu -> **Change runtime type**

Then, we clone the repository, set up the envrironment, and download the pre-trained model.
"""

# Commented out IPython magic to ensure Python compatibility.
# Clone GFPGAN and enter the GFPGAN folder
# %cd /content
!rm -rf GFPGAN
!git clone https://github.com/TencentARC/GFPGAN.git
# %cd GFPGAN

# Set up the environment
# Install basicsr - https://github.com/xinntao/BasicSR
# Set BASICSR_EXT=True to compile the cuda extensions in the BasicSR - It may take several minutes to compile, please be patient.
!BASICSR_EXT=True pip install basicsr
# Install facexlib - https://github.com/xinntao/facexlib
# We use face detection and face restoration helper in the facexlib package
!pip install facexlib
# Install other depencencies
!pip install -r requirements.txt
!python setup.py develop
!pip install realesrgan  # used for enhancing the background (non-face) regions
# Download the pre-trained model
!wget https://github.com/TencentARC/GFPGAN/releases/download/v0.1.0/GFPGANv1.pth -P experiments/pretrained_models

"""# Inference cropped face images

We first look at the cropped low-quality faces.<br>
Some examples are in the <inputs/cropped_faces> folder.
"""

# visulize the cropped low-quality faces
import cv2
import matplotlib.pyplot as plt

def imread(img_path):
  img = cv2.imread(img_path)
  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
  return img

# read images
img1 = imread('inputs/cropped_faces/Adele_crop.png')
img2 = imread('inputs/cropped_faces/Julia_Roberts_crop.png')
img3 = imread('inputs/cropped_faces/Justin_Timberlake_crop.png')
img4 = imread('inputs/cropped_faces/Paris_Hilton_crop.png')

# show images
fig = plt.figure(figsize=(25, 10))
ax1 = fig.add_subplot(1, 4, 1) 
ax1.imshow(img1)
ax1.axis('off')
ax2 = fig.add_subplot(1, 4, 2) 
ax2.imshow(img2)
ax2.axis('off')
ax3 = fig.add_subplot(1, 4, 3) 
ax3.imshow(img3)
ax3.axis('off')
ax4 = fig.add_subplot(1, 4, 4) 
ax4.imshow(img4)
ax4.axis('off')

# Now we use the GFPGAN to restore the above low-quality images
# You can find the different models in https://github.com/TencentARC/GFPGAN#european_castle-model-zoo
!rm -rf results
!python inference_gfpgan.py -i inputs/cropped_faces -o results -v 1 -s 2 --aligned

# Usage: python inference_gfpgan.py -i inputs/whole_imgs -o results -v 1.3 -s 2 [options]...
# 
#  -h                   show this help
#  -i input             Input image or folder. Default: inputs/whole_imgs
#  -o output            Output folder. Default: results
#  -v version           GFPGAN model version. Option: 1 | 1.2 | 1.3. Default: 1.3
#  -s upscale           The final upsampling scale of the image. Default: 2
#  -bg_upsampler        background upsampler. Default: realesrgan
#  -bg_tile             Tile size for background sampler, 0 for no tile during testing. Default: 400
#  -suffix              Suffix of the restored faces
#  -only_center_face    Only restore the center face
#  -aligned             Input are aligned faces
#  -ext                 Image extension. Options: auto | jpg | png, auto means using the same extension as inputs. Default: auto

# Now, let us to look at the reuslts! :-)
# The results are in the <resutls> folder
!ls results

# Visualize the results
# The left are the inputs images; the right are the results of GFPGAN
import cv2
import matplotlib.pyplot as plt

def imread(img_path):
  img = cv2.imread(img_path)
  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
  return img

# read images
img1 = imread('results/cmp/Adele_crop_00.png')
img2 = imread('results/cmp/Julia_Roberts_crop_00.png')
img3 = imread('results/cmp/Justin_Timberlake_crop_00.png')
img4 = imread('results/cmp/Paris_Hilton_crop_00.png')

# show images
fig = plt.figure(figsize=(15, 30))
ax1 = fig.add_subplot(4, 1, 1) 
ax1.imshow(img1)
ax1.axis('off')
ax2 = fig.add_subplot(4, 1, 2) 
ax2.imshow(img2)
ax2.axis('off')
ax3 = fig.add_subplot(4, 1, 3) 
ax3.imshow(img3)
ax3.axis('off')
ax4 = fig.add_subplot(4, 1, 4) 
ax4.imshow(img4)
ax4.axis('off')

"""We can see that:
Not only the **facial details**, but also the **colors** are enhanced by the GFPGAN model.

# Inference whole images

We use `facexlib` to detect and crop face images on a whole photo, and then restore the face images by GFPGAN.

Note that currently, it does not well support to paste the faces back into the original photo, as the restored faces are incosistent with the original photos in terms of details and colors.<br>
You can find the improved version in this [Colab Demo](https://colab.research.google.com/drive/1sVsoBd9AjckIXThgtZhGrHRfFI6UUYOo).

<br>

Now, we will enhance the faces in the following two photos.
"""

# Visualize input images to be resotred
import cv2
import matplotlib.pyplot as plt

def imread(img_path):
  img = cv2.imread(img_path)
  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
  return img

# read images
img1 = imread('inputs/whole_imgs/00.jpg')
img2 = imread('inputs/whole_imgs/10045.png')

# show images
fig = plt.figure(figsize=(25, 10))
ax1 = fig.add_subplot(1, 2, 1) 
ax1.imshow(img1)
ax1.axis('off')
ax2 = fig.add_subplot(1, 2, 2) 
ax2.imshow(img2)
ax2.axis('off')

# Now we use the GFPGAN to restore the above low-quality images
# We use [Real-ESRGAN](https://github.com/xinntao/Real-ESRGAN) for enhancing the background (non-face) regions
# You can find the different models in https://github.com/TencentARC/GFPGAN#european_castle-model-zoo
!rm -rf results
!python inference_gfpgan.py -i inputs/whole_imgs -o results -v 1 -s 2 --bg_upsampler realesrgan

# Usage: python inference_gfpgan.py -i inputs/whole_imgs -o results -v 1.3 -s 2 [options]...
# 
#  -h                   show this help
#  -i input             Input image or folder. Default: inputs/whole_imgs
#  -o output            Output folder. Default: results
#  -v version           GFPGAN model version. Option: 1 | 1.2 | 1.3. Default: 1.3
#  -s upscale           The final upsampling scale of the image. Default: 2
#  -bg_upsampler        background upsampler. Default: realesrgan
#  -bg_tile             Tile size for background sampler, 0 for no tile during testing. Default: 400
#  -suffix              Suffix of the restored faces
#  -only_center_face    Only restore the center face
#  -aligned             Input are aligned faces
#  -ext                 Image extension. Options: auto | jpg | png, auto means using the same extension as inputs. Default: auto

# Visualize the results
# The left are the inputs images; the right are the results of GFPGAN
import cv2
import matplotlib.pyplot as plt

def imread(img_path):
  img = cv2.imread(img_path)
  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
  return img

# read images
img1 = imread('results/cmp/00_00.png')
img2 = imread('results/cmp/00_01.png')
img3 = imread('results/cmp/10045_02.png')
img4 = imread('results/cmp/10045_01.png')

# show images
fig = plt.figure(figsize=(15, 30))
ax1 = fig.add_subplot(4, 1, 1) 
ax1.imshow(img1)
ax1.axis('off')
ax2 = fig.add_subplot(4, 1, 2) 
ax2.imshow(img2)
ax2.axis('off')
ax3 = fig.add_subplot(4, 1, 3) 
ax3.imshow(img3)
ax3.axis('off')
ax4 = fig.add_subplot(4, 1, 4) 
ax4.imshow(img4)
ax4.axis('off')

# Visualize the whole images 
# However, due to the color and detail inconsistency, the results may look unnatural. 

import cv2
import matplotlib.pyplot as plt

def imread(img_path):
  img = cv2.imread(img_path)
  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
  return img

# read images
img1 = imread('results/restored_imgs/00.jpg')
img2 = imread('results/restored_imgs/10045.png')

# show images
fig = plt.figure(figsize=(25, 10))
ax1 = fig.add_subplot(1, 2, 1) 
ax1.imshow(img1)
ax1.axis('off')
ax2 = fig.add_subplot(1, 2, 2) 
ax2.imshow(img2)
ax2.axis('off')

"""You can see that:
1. the faces in the first photo are enhanced and the whole result looks natural;
2. However, as the face colors have been largely changed in the second photo, therefore, the whole result looks inconsistent.

BTW, we use [Real-ESRGAN](https://github.com/xinntao/Real-ESRGAN) for enhancing the background (non-face) regions.

# Try it on uploaded images

## 1. Upload images
"""

import os
from google.colab import files
import shutil

upload_folder = 'inputs/upload'

if os.path.isdir(upload_folder):
    shutil.rmtree(upload_folder)
# os.mkdir('inputs')
os.mkdir(upload_folder)

# upload images
uploaded = files.upload()
for filename in uploaded.keys():
  dst_path = os.path.join(upload_folder, filename)
  print(f'move {filename} to {dst_path}')
  shutil.move(filename, dst_path)

"""## 2. Inference"""

# Now we use the GFPGAN to restore the above low-quality images
# We use [Real-ESRGAN](https://github.com/xinntao/Real-ESRGAN) for enhancing the background (non-face) regions
# You can find the different models in https://github.com/TencentARC/GFPGAN#european_castle-model-zoo
!rm -rf results
!python inference_gfpgan.py -i inputs/upload -o results -v 1 -s 4 --bg_upsampler realesrgan
!ls results/cmp
# Usage: python inference_gfpgan.py -i inputs/whole_imgs -o results -v 1.3 -s 2 [options]...
# 
#  -h                   show this help
#  -i input             Input image or folder. Default: inputs/whole_imgs
#  -o output            Output folder. Default: results
#  -v version           GFPGAN model version. Option: 1 | 1.2 | 1.3. Default: 1.3
#  -s upscale           The final upsampling scale of the image. Default: 2
#  -bg_upsampler        background upsampler. Default: realesrgan
#  -bg_tile             Tile size for background sampler, 0 for no tile during testing. Default: 400
#  -suffix              Suffix of the restored faces
#  -only_center_face    Only restore the center face
#  -aligned             Input are aligned faces
#  -ext                 Image extension. Options: auto | jpg | png, auto means using the same extension as inputs. Default: auto

"""## 3. Visualize"""

# Visualize the results (can safely skip)
# The left are the inputs images; the right are the results of GFPGAN
# (!) This needs updating to deal with uploads instead of default images
import cv2
import matplotlib.pyplot as plt

def imread(img_path):
  img = cv2.imread(img_path)
  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
  return img

# read images
img1 = imread('results/cmp/008_Benedict_Cumberbatch_01.png')
img2 = imread('results/cmp/008_Benedict_Cumberbatch_00.png')


# show images
fig = plt.figure(figsize=(15, 15))
ax1 = fig.add_subplot(2, 1, 1) 
ax1.imshow(img1)
ax1.axis('off')
ax2 = fig.add_subplot(2, 1, 2) 
ax2.imshow(img2)
ax2.axis('off')

"""## 4. Download results"""

# download the result
!ls results
print('Download results')
os.system('zip -r download.zip results')
files.download("download.zip")

"""You may want to try an **imporoved** model without colorization. (It also does not require CUDA extensions). Please go to this [Colab Demo](https://colab.research.google.com/drive/1sVsoBd9AjckIXThgtZhGrHRfFI6UUYOo) for GFPGAN <a href="https://colab.research.google.com/drive/1sVsoBd9AjckIXThgtZhGrHRfFI6UUYOo"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="google colab logo"></a>."""